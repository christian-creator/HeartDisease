Permutation Importance:
    - Describes what features that has the biggest impact in an already generated model.
    - Permutation Importance is calculated by shuffling a column of features and calculating the new predictions given the model.
    - If the predictions given the randomly shuffled feature is way worse the feature is most likely important for the model
    - If the predictions given the randomly shuffled feature is better or did'nt change the feature most likely does not play an important role in the model
    - Permutation Importance is illustrated as an integer +- another integer i.e: 0.0711 ± 0.0542.
        The first number in each row shows how much model performance decreased with a random shuffling
        The number after the ± measures how performance varied from one-reshuffling to the next.

Partial plots:
    - Describes show how a specific feature affects the predictions. (Kinda like coeffecients in linear regression models)
    - Like permutation importance, partial dependence plots are calculated after a model has been fit.
    - Parital plots are generated by repeatedly altering the value of one variable to make a series of predictions.
        The predicted outcome for each value change is then plotted. Predicted outcome on the y-axis and feature change on the x-axis.
        If the Predicted outcome increases the feature has a positive influence in the predicted outcome and vise verca.
    - Typically compared to a baseline predictions which is the lowest value of the feature
    2D Partial Dependence Plots:
        -   2D Partial Dependence Plots can show how nteractions between features influence the predictions

Shap values:
    - The goal of SHAP is to explain the prediction of an instance x by computing the contribution of each feature to the prediction.
    - SHAP values interpret the impact of having a certain value for a given feature in comparison to the prediction we'd make if that feature took some baseline value.
        * How much was a prediction of man of the match driven by the fact that the team scored 3 goals, instead of some baseline number of goals.
        calculation = sum(SHAP values for all features) = pred_for_team - pred_for_baseline_values
        That is, the SHAP values of all features sum up to explain why my prediction was different from the baseline. This allows us to decompose a prediction in a graph
    - The prediction starts from the baseline. The baseline for Shapley values is the average of all predictions.
    - In the plot, each Shapley value is an arrow that pushes to increase (positive value) or decrease (negative value) the prediction.
